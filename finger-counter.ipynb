{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idris\\Anaconda3\\lib\\site-packages\\skimage\\viewer\\utils\\core.py:10: UserWarning: Recommended matplotlib backend is `Agg` for full skimage.viewer functionality.\n",
      "  warn(\"Recommended matplotlib backend is `Agg` for full \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "import shutil\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sorting the files into correct folders based on name on my local computer\n",
    "\n",
    "#for folder in ('train', 'test'):\n",
    "folder = 'train'\n",
    "unsorted_img_paths = glob.glob('I:/count_fingers_cnn/fingers_2/{}/*.png'.format(folder))\n",
    "for img_path in unsorted_img_paths:\n",
    "    shutil.move(img_path, 'I:/count_fingers_cnn/fingers_2/{}/'.format(folder)+img_path[-5:-4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "num_classes = 12\n",
    "batch_size = 96\n",
    "epochs = 6\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(2,2), activation='elu', padding='same', input_shape=(img_size, img_size, 1)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='elu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='elu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='elu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='elu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 64)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 673,996\n",
      "Trainable params: 673,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# turn an array of prediction chances into a single value of the highest probability prediction\n",
    "def decode_prediction(pred): \n",
    "    #classes = ['0','1','2','3','4','5']\n",
    "    classes = ['0L', '0R', '1L', '1R', '2L', '2R', '3L', '3R', '4L', '4R', '5L', '5R']\n",
    "    pred = list(pred)\n",
    "    return classes[pred.index(max(pred))]\n",
    "\n",
    "# same as above but for multiple predictions of different hands at once\n",
    "def decode_predictions(preds):\n",
    "    return [decode_prediction(pred) for pred in preds]\n",
    "\n",
    "# predict with a model from an image variable\n",
    "def predict_from_image(model, img):\n",
    "    img_arr = np.array([img_to_array(img)])\n",
    "    preds = model.predict(img_arr)\n",
    "    output = decode_prediction(preds[0])\n",
    "    print(preds[0])\n",
    "    return output\n",
    "\n",
    "# predict with a model from an image file path\n",
    "def predict_from_image_path(model, img_path):\n",
    "    img = load_img(img_path, target_size=(img_size, img_size), color_mode='grayscale')\n",
    "    return predict_from_image(model, img)\n",
    "\n",
    "# convert a cv image/array to a PIL image\n",
    "def convert_cv_to_pil(img_cv):\n",
    "    #img_cv = cv.cvtColor(img_cv, cv.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_cv)\n",
    "    return img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 images belonging to 12 classes.\n",
      "Found 3600 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Getting data from the directories\n",
    "\n",
    "train_data_gen = ImageDataGenerator(\n",
    "                                    #horizontal_flip = True,\n",
    "                                    width_shift_range = 0.05,\n",
    "                                    height_shift_range = 0.05,\n",
    "                                    zoom_range = 0.05,\n",
    "                                    rotation_range = 20,\n",
    "                                    #validation_split = 0.2\n",
    "                                    )\n",
    "\n",
    "val_data_gen = ImageDataGenerator() \n",
    "\n",
    "train_gen = train_data_gen.flow_from_directory(directory='I:/count_fingers_cnn/fingers/train', # CHANGE TO 'fingers' DIRECTORY\n",
    "                                              target_size=(img_size,img_size),\n",
    "                                              color_mode='grayscale',\n",
    "                                              class_mode='categorical',\n",
    "                                              batch_size=batch_size\n",
    "                                              #subset = 'training'\n",
    "                                              )\n",
    "\n",
    "\n",
    "val_gen = val_data_gen.flow_from_directory(directory='I:/count_fingers_cnn/fingers/test',  # CHANGE TO 'fingers' DIRECTORY\n",
    "                                           target_size=(img_size,img_size), \n",
    "                                           color_mode='grayscale',\n",
    "                                           class_mode='categorical',\n",
    "                                           batch_size=batch_size\n",
    "                                           #subset = 'validation'\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "188/188 [==============================] - 53s 282ms/step - loss: 3.1821 - acc: 0.4849 - val_loss: 0.3251 - val_acc: 0.8667\n",
      "Epoch 2/6\n",
      "188/188 [==============================] - 49s 262ms/step - loss: 0.1870 - acc: 0.9334 - val_loss: 0.0854 - val_acc: 0.9689\n",
      "Epoch 3/6\n",
      "188/188 [==============================] - 50s 266ms/step - loss: 0.0836 - acc: 0.9722 - val_loss: 0.0229 - val_acc: 0.9928\n",
      "Epoch 4/6\n",
      "188/188 [==============================] - 51s 269ms/step - loss: 0.0466 - acc: 0.9850 - val_loss: 0.0123 - val_acc: 0.9958\n",
      "Epoch 5/6\n",
      "188/188 [==============================] - 53s 284ms/step - loss: 0.0382 - acc: 0.9877 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 6/6\n",
      "188/188 [==============================] - 51s 273ms/step - loss: 0.0320 - acc: 0.9899 - val_loss: 7.9302e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d7a1e86898>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model with the parameters\n",
    "\n",
    "model.fit(train_gen,\n",
    "          epochs=epochs,\n",
    "          validation_data=val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0000000e+00 0.0000000e+00 7.3246123e-32 2.9341125e-36 1.7067546e-18\n",
      " 7.8358065e-23 2.6730524e-07 5.7289306e-15 9.9999976e-01 2.3239431e-11\n",
      " 2.1264503e-10 8.3831232e-15]\n",
      "4L\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on different hands from files on my local computer\n",
    "\n",
    "\n",
    "#random_img_path = '../input/fingers-formatted/test/5R/01077f28-4d49-4fb0-bcdf-74a11d41214d_5R.png'\n",
    "#print(predict_from_image_path(model, random_img_path))\n",
    "\n",
    "#test_finger_path = 'I:/count_fingers_cnn/my_fingers/2R.jpg'\n",
    "#test_finger_path = 'I:/count_fingers_cnn/fingers/test/4L/0a678a65-ec44-4ff4-a92e-280d5183e98b_4L.png'\n",
    "#test_finger_path = 'I:/count_fingers_cnn/fingers_2/train/1/0_1.png'\n",
    "\n",
    "#test_finger_img = load_img(test_finger_path, target_size=(img_size, img_size), color_mode = 'grayscale')\n",
    "#test_finger_img = prep_image_from_path(test_finger_path)\n",
    "\n",
    "#test_finger_img.show()\n",
    "\n",
    "# testing model from saved file\n",
    "#model2 = tf.keras.models.load_model('I:/count_fingers_cnn/v2-desktop/model.pb')\n",
    "#print(predict_from_image_path(model2, test_finger_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to a file\n",
    "\n",
    "#model.save('I:/count_fingers_cnn/v2-desktop/model.pb')\n",
    "#model_from_file = tf.keras.models.load_model('I:/count_fingers_cnn/v2-desktop/model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
